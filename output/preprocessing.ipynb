{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook we will read in the train videos and extract n number of frames. Then we will perform facial recognition to extract every face from those frames and write them as their own images (after resizing). \n",
    "\n",
    "No augmentation will be done in this notebook - this will leave us the option to do it after the raw face images have been written. That way we can try numerous augmentation techniques without having to extract the frames again, and ensures that we try augmentation to the same raw images each time (and thus have a more reliable testing environment).\n",
    "\n",
    "We use OpenCV to read the videos, extract the frames and reshape them. The [MTCNN algorithm](https://github.com/ipazc/mtcnn) is used for facial recognition. This is an effective algorithm, however I am keen to try quicker, more lightweight algorithms like BlazeFace and YOLOv2 later on...\n",
    "\n",
    "------------------------------\n",
    "*PLEASE NOTE*:\n",
    "The scripts in this notebook have been designed for the FULL training dataset on [Kaggle](https://www.kaggle.com/c/deepfake-detection-challenge). There will be some pathing and folder related errors if you attempt this using the train_sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install ../input/mtcnn-package/mtcnn-0.1.0-py3-none-any.whl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our directory paths and directory lists - including the directory where we will save our train and test images that we extract from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos_path = '../input/train_videos/'\n",
    "train_metadata_path = '../input/train_metadata/'\n",
    "train_images_path = \"../input/train_images/\" # path to save train images to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop through all of the videos in all the train folder locations to make one list of paths. We will also rename the metadata (to determine which folder it corresponds to) and copy it to a new directory 'train_metadata'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos_files = [] # List of all train videos paths\n",
    "train_metadata_files = [] # List of train metadata paths\n",
    "\n",
    "for folder in enumerate(os.listdir(train_videos_path)):\n",
    "    for file in os.listdir(train_videos_path + folder[1]):\n",
    "        if file == 'metadata.json':\n",
    "            # Rename and copy the metadata to a new directory\n",
    "            old_path = train_videos_path + folder[1] + '/' + file\n",
    "            new_path = train_metadata_path + 'metadata' + str(folder[0]) + '.json'\n",
    "            shutil.copy(old_path, new_path)            \n",
    "            train_metadata_files.append(new_path)\n",
    "        else:\n",
    "            train_videos_files.append(train_videos_path + folder[1] + '/'+ file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop round all the videos in our directory to extract images for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(videos_dir_path, images_dir_path, frames=1, conf_level=0.75):\n",
    "    \"\"\"\n",
    "    Inputs a directory of videos, extracts n frames. \n",
    "    Outputs images of ANY faces detected in those frames.\n",
    "        \n",
    "    videos_dir_path: (str) Path to your directory of videos\n",
    "    images_dir_path: (str) Path to where you'll save your images to\n",
    "    frames: (int) Extract n number of random frames from video.\n",
    "    \"\"\"\n",
    "    if type(videos_dir_path) == list: \n",
    "        videos_dir = videos_dir_path\n",
    "    else: \n",
    "        videos_dir = os.listdir(videos_dir_path) # List train vids\n",
    "    # Extract images from videos\n",
    "    print(f'Extracting {frames} random frame(s) from videos')\n",
    "    detector = MTCNN() # Facial recognition algorithm\n",
    "    \n",
    "    for video in tqdm(range(0, len(videos_dir))):\n",
    "        file_name = videos_dir_path[video].split('/')[4]\n",
    "        file_path = videos_dir_path[video]\n",
    "        vid_name = file_name.split('.')[0]\n",
    "        frames_list = [] # We'll store the raw frames here\n",
    "        for i in range(0, frames):\n",
    "            try:\n",
    "                # Extract frames from video \n",
    "                cap = cv2.VideoCapture(file_path)\n",
    "                total_frames = cap.get(7)\n",
    "                frame_number = random.randint(0, total_frames)\n",
    "                cap.set(1, frame_number)\n",
    "                success, frame = cap.read()\n",
    "                frames_list.append(frame)\n",
    "                \n",
    "                \n",
    "                ## USE BELOW FOR TESTING - IS NOT SCALABLE FOR TRAIN EXTRACTION ## \n",
    "                \n",
    "                ## Number to get every nth frame from. E.g. if we want\n",
    "                ## 10 frames, and the video has 300 total frames, \n",
    "                ## we take every 30th frame.\n",
    "                #num_frames = total_frames / frames \n",
    "                #success, frame = cap.read()\n",
    "                #count = 0\n",
    "                #while success:\n",
    "                #    if count % num_frames == 0:\n",
    "                #        frames_list.append(frame)\n",
    "                #    success, frame = cap.read()\n",
    "                #    count += 1\n",
    "                #    if count > num_frames:\n",
    "                #        break\n",
    "                \n",
    "                ## USE ABOVE FOR TESTING - IS NOT SCALABLE FOR TRAIN EXTRACTION ## \n",
    "                \n",
    "                \n",
    "                \n",
    "                # Extract faces from frames\n",
    "                for i, image in enumerate(frames_list):\n",
    "                    frame_name = vid_name + '_' + str(i)\n",
    "                    # Read image and detect faces\n",
    "                    result = detector.detect_faces(image)\n",
    "                    # Extract and save faces as their own images\n",
    "                    faces = []\n",
    "                    for i, face in enumerate(result):\n",
    "                        # Only extract the face if confidence is more than or equal to default 0.95\n",
    "                        if face['confidence'] >= conf_level:\n",
    "                            startX, startY, width, height = face['box'] # Get box coordinates\n",
    "                            img_crop = image[startY:startY + height, startX:startX + width]\n",
    "                            img_crop_resize = cv2.resize(img_crop, (256, 256))\n",
    "        \n",
    "                            crop_img_name = images_dir_path + frame_name + '_' + str(i) + '.jpg'\n",
    "                            cv2.imwrite(crop_img_name, img_crop_resize)\n",
    "                        else:\n",
    "                            pass\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 1 random frame(s) from videos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca915be57fbd46f89a87e749b8374883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=119146.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_faces(train_videos_files, train_images_path, frames=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the face extraction and image preprocessing stage for the training data. We should now have a directory of images that we will train our model with.\n",
    "\n",
    "This is by no means a perfect solution - it took ~1.5 days to complete on the entire training set. I ended up with ~240,000 images. This is with GPU. \n",
    "\n",
    "Ideally I wouldn't have to loop through the whole video just to extract n frames (which is where the significant time comes from). However I haven't found a way so far that can just pluck out the desired frames - and not through lack of trying!! Video codecs are just too complicated and OpenCV is not yet equipped to have this functionality.\n",
    "\n",
    "We'll revisit this code in the test stage, when we create our test pipeline.\n",
    "\n",
    "The next stage of this project is the [Train notebook](https://github.com/TheNerdyCat/deepfake-detection-challenge/blob/master/output/train.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
