{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook we will read in the train videos and extract n number of frames. Then we will perform facial recognition to extract every face from those frames and write them as their own images (after resizing). \n",
    "\n",
    "No augmentation will be done in this notebook - this will leave us the option to do it after the raw face images have been written. That way we can try numerous augmentation techniques without having to extract the frames again, and ensures that we try augmentation to the same raw images each time (and thus have a more reliable testing environment).\n",
    "\n",
    "We use OpenCV to read the videos, extract the frames and reshape them. The [MTCNN algorithm](https://github.com/ipazc/mtcnn) is used for facial recognition. This is an effective algorithm, however I am keen to try quicker, more lightweight algorithms like BlazeFace and YOLOv2 later on...\n",
    "\n",
    "------------------------------\n",
    "*PLEASE NOTE*:\n",
    "The scripts in this notebook have been designed for the FULL training dataset on [Kaggle](https://www.kaggle.com/c/deepfake-detection-challenge). There will be some pathing and folder related errors if you attempt this using the train_sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!pip install ../input/mtcnn-package/mtcnn-0.1.0-py3-none-any.whl\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define our directory paths and directory lists - including the directory where we will save our train and test images that we extract from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos_path = '../input/train_videos/'\n",
    "train_metadata_path = '../input/train_metadata/'\n",
    "train_images_path = \"../input/train_images/\" # path to save train images to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll loop through all of the videos in all the train folder locations to make one list of paths. We will also rename the metadata (to determine which folder it corresponds to) and copy it to a new directory 'train_metadata'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos_files = [] # List of all train videos paths\n",
    "train_metadata_files = [] # List of train metadata paths\n",
    "\n",
    "for folder in enumerate(os.listdir(train_videos_path)):\n",
    "    for file in os.listdir(train_videos_path + folder[1]):\n",
    "        if file == 'metadata.json':\n",
    "            # Rename and copy the metadata to a new directory\n",
    "            old_path = train_videos_path + folder[1] + '/' + file\n",
    "            new_path = train_metadata_path + 'metadata' + str(folder[0]) + '.json'\n",
    "            shutil.copy(old_path, new_path)            \n",
    "            train_metadata_files.append(new_path)\n",
    "        else:\n",
    "            train_videos_files.append(train_videos_path + folder[1] + '/'+ file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop round all the videos in our directory to extract images for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(videos_dir_path, images_dir_path, frames=1, conf_level=0.95):\n",
    "    \"\"\"\n",
    "    Inputs a directory of videos, extracts n frames. \n",
    "    Outputs images of ANY faces detected in those frames.\n",
    "    \n",
    "    videos_dir_path: (str) Path to your directory of videos\n",
    "    images_dir_path: (str) Path to where you'll save your images to\n",
    "    frames: (int or list) Number of frames. If int, take that many frames. If list, take frame numbers specified in list. \n",
    "    conf_level: (float) Confidence level for the face recognition model.\n",
    "    \"\"\"\n",
    "    def crop(img, x, y, w, h):\n",
    "        \"\"\"\n",
    "        Crop and reshape images to be uniform across all frames\n",
    "        \"\"\"\n",
    "        x -= 40\n",
    "        y -= 40\n",
    "        w += 80\n",
    "        h += 80\n",
    "        if x < 0:\n",
    "            x = 0\n",
    "        if y <= 0:\n",
    "            y = 0\n",
    "        return cv2.cvtColor(cv2.resize(img[y:y + h, x:x + w], (256, 256)), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if type(videos_dir_path) == list: \n",
    "        videos_dir = videos_dir_path\n",
    "    else: \n",
    "        videos_dir = os.listdir(videos_dir_path) # List train vids\n",
    "    \n",
    "    # Extract images from videos\n",
    "    if type(frames) == list:\n",
    "        print(f'Extracting frames {frames} from videos')\n",
    "    else:\n",
    "        print(f'Extracting {frames} random frame(s) from videos')\n",
    "        \n",
    "    with tqdm(total=len(range(0, len(videos_dir)))) as pbar: \n",
    "        for i in range(0, len(videos_dir)): \n",
    "            try:\n",
    "                if type(videos_dir_path) == list: \n",
    "                    file_name = videos_dir_path[i].split('/')[4]\n",
    "                    file_path = videos_dir_path[i]\n",
    "                    vid_name = file_name.split('.')[0]\n",
    "                else: \n",
    "                    file_name = videos_dir[i] # file name with .ext\n",
    "                    file_path = videos_dir_path + file_name # full file path\n",
    "                    vid_name = file_name.split('.')[0] # file name without .ext\n",
    "\n",
    "                if type(frames) == list:\n",
    "                    for num in range(0, len(frames)):\n",
    "                        cap = cv2.VideoCapture(file_path)\n",
    "                        total_frames = cap.get(7)\n",
    "                        vid_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                        cap.set(1, num) # EDIT HERE FOR FRAME NUMBER\n",
    "                        ret, frame = cap.read()\n",
    "                        image_name = vid_name + '_' + str(num) + '.jpg'\n",
    "                        cv2.imwrite(os.path.join(train_images_path, image_name), frame) # Save frame as image\n",
    "                        cv2.destroyAllWindows()\n",
    "                        cap.release()\n",
    "                else:\n",
    "                    for num in range(0, frames):\n",
    "                        cap = cv2.VideoCapture(file_path)\n",
    "                        total_frames = cap.get(7)\n",
    "                        vid_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                        cap.set(1, random.randint(0, vid_length)) # EDIT HERE FOR FRAME NUMBER\n",
    "                        ret, frame = cap.read()\n",
    "                        image_name = vid_name + '_' + str(num) + '.jpg'\n",
    "                        cv2.imwrite(os.path.join(train_images_path, image_name), frame) # Save frame as image\n",
    "                        cv2.destroyAllWindows()\n",
    "                        cap.release()\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                pass\n",
    "    images_dir = os.listdir(images_dir_path) # List newly created training images\n",
    "    detector = MTCNN()\n",
    "\n",
    "    print('Extracting faces from frames')\n",
    "    with tqdm(total=len(images_dir)) as pbar:\n",
    "        \n",
    "        for image in range(0, len(images_dir)):\n",
    "            try:\n",
    "                image_name = images_dir[image].split('.')[0] # Get image name without .ext\n",
    "             # Read image and detect faces\n",
    "                frame = cv2.imread(images_dir_path + images_dir[image])\n",
    "                result = detector.detect_faces(frame)\n",
    "             # Extract and save faces as their own images\n",
    "                for face in range(0, len(result)):\n",
    "                    # Only extract the face if confidence is more than or equal to default 0.95\n",
    "                    if result[face]['confidence'] >= conf_level:            \n",
    "                        startX, startY, endX, endY = result[face]['box'] # Get box coordinates\n",
    "                        crop_img = crop(frame, startX, startY, endX, endY)\n",
    "                        cv2.imwrite(images_dir_path + image_name + '_' + str(face) + '.jpg', crop_img)\n",
    "                os.remove(images_dir_path + images_dir[image]) # Delete original images\n",
    "            except:\n",
    "                pass\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 2 random frames from videos\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb75fc276de46c0b7a6c78db3e9ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Edward Sims\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Extracting faces from frames\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bc2f6ac20a42aa9e6015cb473a058f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_faces(train_videos_files, train_images_path, frames=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have completed the face extraction and image preprocessing stage for the training data. We should now have a directory of images that we will train our model with.\n",
    "\n",
    "This is by no means a perfect solution - it took ~1 day to complete on the entire training set. I ended up with ~120,000 images. \n",
    "\n",
    "I'd like to find a way to not have to write the images twice (once for the frame extraction, and once for the face extraction) but have not yet found a solution. I will look into this aat a later date.\n",
    "\n",
    "We'll revisit this code in the test stage, when we create our test pipeline.\n",
    "\n",
    "The next stage of this project is the [Train notebook](https://github.com/TheNerdyCat/deepfake-detection-challenge/blob/master/output/train.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
