{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Following from [Preprocessing](https://github.com/TheNerdyCat/deepfake-detection-challenge/blob/master/output/preprocessing.ipynb), this stage will look at data augmentation and subsequently training the model.\n",
    "\n",
    "We will read our extracted faces using OpenCV and perform any data augmentation. Following this, we will define X and X_val. Then we'll read the metadata to label the extracted faces as FAKE or REAL, defining them into y and y_val.\n",
    "\n",
    "After we have our training data and validation data ready and shuffled, we'll train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json # To read the metadata\n",
    "\n",
    "import keras\n",
    "from keras import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_images_path = '../input/train_images/'\n",
    "train_sample_images = os.listdir(train_sample_images_path)\n",
    "\n",
    "# Read in metadata\n",
    "with open('../input/train_metadata/metadata0.json') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "X = []\n",
    "for img in train_sample_images:\n",
    "    img = train_sample_images_path + img\n",
    "    img = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
    "    X.append(img)\n",
    "\n",
    "y = []\n",
    "for label in train_sample_images:\n",
    "    if metadata[label.split('_')[0] + '.mp4']['label'] == 'REAL':\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling to go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    new_train = []\n",
    "    for m, n in zip(X, y):\n",
    "        new_train.append([m, n])\n",
    "    random.shuffle(new_train)\n",
    "    X, y = [], []\n",
    "    for x in new_train:\n",
    "        X.append(x[0])\n",
    "        y.append(x[1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "\n",
    "# Create X_val from 10% of X\n",
    "X_val = X[:round(len(X) / 100 * 25)]\n",
    "X = X[round(len(X) / 100 * 25):]\n",
    "\n",
    "# Create y_val from 10% of y\n",
    "y_val = y[:round(len(y) / 100 * 25)]\n",
    "y = y[round(len(y) / 100 * 25):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y)\n",
    "X_val, y_val = shuffle(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionLayer(a, b, c, d):\n",
    "    def func(x):\n",
    "        x1 = Conv2D(a, (1, 1), padding='same', activation='elu')(x)\n",
    "        \n",
    "        x2 = Conv2D(b, (1, 1), padding='same', activation='elu')(x)\n",
    "        x2 = Conv2D(b, (3, 3), padding='same', activation='elu')(x2)\n",
    "            \n",
    "        x3 = Conv2D(c, (1, 1), padding='same', activation='elu')(x)\n",
    "        x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides=1, padding='same', activation='elu')(x3)\n",
    "        \n",
    "        x4 = Conv2D(d, (1, 1), padding='same', activation='elu')(x)\n",
    "        x4 = Conv2D(d, (3, 3), dilation_rate=3, strides=1, padding='same', activation='elu')(x4)\n",
    "        y = Concatenate(axis = -1)([x1, x2, x3, x4])\n",
    "            \n",
    "        return y\n",
    "    return func\n",
    "    \n",
    "def define_model(shape=(256, 256, 3)):\n",
    "    x = Input(shape=shape)\n",
    "    \n",
    "    x1 = InceptionLayer(1, 4, 4, 2)(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "    \n",
    "    x2 = InceptionLayer(2, 4, 4, 2)(x1)\n",
    "    x2 = BatchNormalization()(x2)        \n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "        \n",
    "    x3 = Conv2D(16, (5, 5), padding='same', activation='elu')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "        \n",
    "    x4 = Conv2D(16, (5, 5), padding='same', activation='elu')(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "    \n",
    "    y = Flatten()(x4)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(16)(y)\n",
    "    y = LeakyReLU(alpha=0.1)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1, activation='sigmoid')(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4))\n",
    "    return model\n",
    "\n",
    "df_model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [1e-3, 5e-4, 1e-4]\n",
    "def schedule(epoch):\n",
    "    return lrs[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 5\n",
    "losses = []\n",
    "\n",
    "models = []\n",
    "i = 0\n",
    "while len(models) < kfolds:\n",
    "    model = define_model()\n",
    "    model.fit([X], [y], epochs=2, callbacks=[LearningRateScheduler(schedule)])\n",
    "    pred = model.predict([X_val])\n",
    "    loss = log_loss(y_val, pred)\n",
    "    losses.append(loss)\n",
    "    print('Fold ' + str(i) + ' model loss: ' + str(loss))\n",
    "    if loss < 0.68:\n",
    "        models.append(model)\n",
    "    else:\n",
    "        print('##############')\n",
    "        print('Retraining')\n",
    "        print('##############')\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
