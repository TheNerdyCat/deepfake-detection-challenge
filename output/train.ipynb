{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Following from [Preprocessing](https://github.com/TheNerdyCat/deepfake-detection-challenge/blob/master/output/preprocessing.ipynb), this stage will look at data augmentation and subsequently training the model.\n",
    "\n",
    "First we will undersample the images to balance REAL and FAKE images in both the train and validation sets. There are actually more FAKE images than REAL in this dataset, so this will be addressed accordingly.\n",
    "\n",
    "We will read our extracted faces using OpenCV and perform any data augmentation. Following this, we will define X and X_val. Then we'll read the metadata to label the extracted faces as FAKE or REAL, defining them into y and y_val.\n",
    "\n",
    "After we have our training data and validation data ready and shuffled, we'll train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json # To read the metadata\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "#import torch\n",
    "#import keras\n",
    "#from keras import Model, Sequential\n",
    "#from keras.layers import *\n",
    "#from keras.optimizers import *\n",
    "#from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True) # Enable GPU logging\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = '../input/train_images/'\n",
    "train_images = os.listdir(train_images_path)\n",
    "metadata_path = '../input/train_metadata/'\n",
    "metadata_dir = os.listdir(metadata_path)\n",
    "\n",
    "# Read in all the metadata files to make one inclusive dict\n",
    "metadata = {}\n",
    "for i, file in enumerate(metadata_dir):\n",
    "    with open('../input/train_metadata/' + file) as json_file:\n",
    "        metadata = {**metadata, **json.load(json_file)}\n",
    "\n",
    "X_paths = []\n",
    "for img in train_images:\n",
    "    img = train_images_path + img\n",
    "    X_paths.append(img)\n",
    "\n",
    "y = []\n",
    "for label in train_images:\n",
    "    if metadata[label.split('_')[0] + '.mp4']['label'] == 'REAL':\n",
    "        y.append(0)\n",
    "    else:\n",
    "        y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    new_train = []\n",
    "    for m, n in zip(X, y):\n",
    "        new_train.append([m, n])\n",
    "    random.shuffle(new_train)\n",
    "    X, y = [], []\n",
    "    for x in new_train:\n",
    "        X.append(x[0])\n",
    "        y.append(x[1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_paths, y = shuffle(X_paths, y)\n",
    "\n",
    "# Create X_test from 10% of X\n",
    "X_test_paths = X_paths[:round(len(X_paths) / 100 * 25)]\n",
    "X_paths = X_paths[round(len(X_paths) / 100 * 25):]\n",
    "\n",
    "# Create y_test from 10% of y\n",
    "y_test = y[:round(len(y) / 100 * 25)]\n",
    "y = y[round(len(y) / 100 * 25):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_paths, y = shuffle(X_paths, y)\n",
    "X_test_paths, y_test = shuffle(X_test_paths, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ' + str(y.count(1)) + ' fake train samples')\n",
    "print('There are ' + str(y.count(0)) + ' real train samples')\n",
    "print('There are ' + str(y_test.count(1)) + ' fake test samples')\n",
    "print('There are ' + str(y_test.count(0)) + ' real test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "Next we'll balance our data, using undersampling techniques. Source for this method can be found [here](https://www.kaggle.com/unkownhihi/starter-kernel-with-cnn-model-ll-lb-0-69235#Apply-Underbalancing-Techinique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "fake = []\n",
    "for m, n in zip(X_paths, y):\n",
    "    if n == 0:\n",
    "        real.append(m)\n",
    "    else:\n",
    "        fake.append(m)\n",
    "fake = random.sample(fake, len(real))\n",
    "X_paths, y = [], []\n",
    "for x in real:\n",
    "    X_paths.append(x)\n",
    "    y.append(0)\n",
    "for x in fake:\n",
    "    X_paths.append(x)\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "fake = []\n",
    "for m, n in zip(X_test_paths, y_test):\n",
    "    if n == 0:\n",
    "        real.append(m)\n",
    "    else:\n",
    "        fake.append(m)\n",
    "fake = random.sample(fake, len(real))\n",
    "X_test_paths, y_test = [], []\n",
    "for x in real:\n",
    "    X_test_paths.append(x)\n",
    "    y_test.append(0)\n",
    "for x in fake:\n",
    "    X_test_paths.append(x)\n",
    "    y_test.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_paths, y = shuffle(X_paths, y)\n",
    "X_test_paths, y_test = shuffle(X_test_paths, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are ' + str(y.count(1)) + ' fake train samples')\n",
    "print('There are ' + str(y.count(0)) + ' real train samples')\n",
    "print('There are ' + str(y_test.count(1)) + ' fake test samples')\n",
    "print('There are ' + str(y_test.count(0)) + ' real test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation will go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 64\n",
    "COLS = 64\n",
    "CHANNELS = 3\n",
    "CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    return cv2.resize(img, (ROWS, COLS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def prepare_data(images):\n",
    "    m = len(images)\n",
    "    X = np.zeros((m, ROWS, COLS, CHANNELS), dtype=np.uint8)\n",
    "    y = np.zeros((1, m), dtype=np.uint8)\n",
    "    for i, image_file in enumerate(images):\n",
    "        X[i,:] = read_image(image_file)\n",
    "         \n",
    "        if metadata[image_file.split('/')[3].split('_')[0]+'.mp4']['label'] == 'REAL':\n",
    "            y[0, i] = 1\n",
    "        elif metadata[image_file.split('/')[3].split('_')[0]+'.mp4']['label'] == 'FAKE':\n",
    "            y[0, i] = 0\n",
    "    return X, y\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = prepare_data(X_paths)\n",
    "test_set_x, test_set_y = prepare_data(X_test_paths)\n",
    "\n",
    "X_train = train_set_x / 255\n",
    "X_test = test_set_x / 255\n",
    "\n",
    "Y_train = convert_to_one_hot(train_set_y, CLASSES).T\n",
    "Y_test = convert_to_one_hot(test_set_y, CLASSES).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"Number of training examples =\", X_train.shape[0])\n",
    "print (\"Number of test examples =\", X_test.shape[0])\n",
    "print (\"X_train shape:\", X_train.shape)\n",
    "print (\"Y_train shape:\", Y_train.shape)\n",
    "print (\"X_test shape:\", X_test.shape)\n",
    "print (\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We implement our ResNet using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value. We'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1,1), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1,1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1,1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, filters, stage, block, s=2):\n",
    "    # defining name basis\n",
    "    conv_name_base='res' + str(stage) + block + '_branch'\n",
    "    bn_name_base='bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides=(s,s), name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
    "\n",
    "    \n",
    "    ##### SHORTCUT PATH ####\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides=(s,s), name = conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet50(input_shape = (64, 64, 3), classes=2):   \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    # Stage 3\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL.\n",
    "    X = AveragePooling2D((2, 2), name='avg_pool')(X)\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = 5\n",
    "kf = KFold(n_splits=kfolds)\n",
    "losses = []\n",
    "\n",
    "for fold, (tdx, vdx) in enumerate(kf.split(X_train, Y_train)):\n",
    "    print(f'Fold : {fold}')\n",
    "    X, X_val, Y, Y_val = X_train[tdx], X_train[vdx], Y_train[tdx], Y_train[vdx]\n",
    "    model = ResNet50(input_shape=(64, 64, 3), classes=2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    es = EarlyStopping(monitor='loss', \n",
    "                   mode='min',\n",
    "                   restore_best_weights=True, \n",
    "                   verbose=2, \n",
    "                   patience=5)\n",
    "    model.fit(X_train, Y_train, callbacks=[es], epochs=10, batch_size=64, verbose=1)\n",
    "    pred = model.predict([X_val])\n",
    "    loss = log_loss(Y_val, pred)\n",
    "    model.save_weights(f'resnet50_{fold}.h5')\n",
    "    print('')\n",
    "    print('Fold ' + str(fold) + ' log loss: ' + str(loss))\n",
    "    print('')\n",
    "    losses.append(loss)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape=(64, 64, 3), classes=2)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "models = []\n",
    "for i in range(kfolds):\n",
    "    model = ResNet50(input_shape=(64, 64, 3), classes=2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.load_weights('../output/resnet50_0.h5')\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.mean([model.predict(X_test[5:5]) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[5:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL 1 - IGNORE FOR NOW ##\n",
    "\n",
    "#def InceptionLayer(a, b, c, d):\n",
    "#    def func(x):\n",
    "#        x1 = Conv2D(a, (1, 1), padding='same', activation='elu')(x)\n",
    "#        \n",
    "#        x2 = Conv2D(b, (1, 1), padding='same', activation='elu')(x)\n",
    "#        x2 = Conv2D(b, (3, 3), padding='same', activation='elu')(x2)\n",
    "#            \n",
    "#        x3 = Conv2D(c, (1, 1), padding='same', activation='elu')(x)\n",
    "#        x3 = Conv2D(c, (3, 3), dilation_rate = 2, strides=1, padding='same', activation='elu')(x3)\n",
    "#        \n",
    "#        x4 = Conv2D(d, (1, 1), padding='same', activation='elu')(x)\n",
    "#        x4 = Conv2D(d, (3, 3), dilation_rate=3, strides=1, padding='same', activation='elu')(x4)\n",
    "#        y = Concatenate(axis=-1)([x1, x2, x3, x4])\n",
    "#            \n",
    "#        return y\n",
    "#    return func\n",
    "#    \n",
    "#def define_model(shape=(256, 256, 3)):\n",
    "#    x = Input(shape=shape)\n",
    "#    \n",
    "#    x1 = InceptionLayer(1, 4, 4, 2)(x)\n",
    "#    x1 = BatchNormalization()(x1)\n",
    "#    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "#    \n",
    "#    x2 = InceptionLayer(2, 4, 4, 2)(x1)\n",
    "#    x2 = BatchNormalization()(x2)        \n",
    "#    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)        \n",
    "#        \n",
    "#    x3 = Conv2D(16, (5, 5), padding='same', activation='elu')(x2)\n",
    "#    x3 = BatchNormalization()(x3)\n",
    "#    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "#        \n",
    "#    x4 = Conv2D(16, (5, 5), padding='same', activation='elu')(x3)\n",
    "#    x4 = BatchNormalization()(x4)\n",
    "#    x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "#    \n",
    "#    y = Flatten()(x4)\n",
    "#    y = Dropout(0.5)(y)\n",
    "#    y = Dense(16)(y)\n",
    "#    y = LeakyReLU(alpha=0.1)(y)\n",
    "#    y = Dropout(0.5)(y)\n",
    "#    y = Dense(1, activation='sigmoid')(y)\n",
    "#    model = Model(inputs=x, outputs=y)\n",
    "#    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4))\n",
    "#    return model\n",
    "#\n",
    "#df_model = define_model()\n",
    "#lrs = [1e-3, 5e-4, 1e-4]\n",
    "#def schedule(epoch):\n",
    "#    return lrs[epoch]\n",
    "#kfolds = 5\n",
    "#losses = []\n",
    "#\n",
    "#models = []\n",
    "#i = 0\n",
    "#while len(models) < kfolds:\n",
    "#    model = define_model()\n",
    "#    model.fit([X], [y], epochs=2, callbacks=[LearningRateScheduler(schedule)])\n",
    "#    pred = model.predict([X_val])\n",
    "#    loss = log_loss(y_val, pred)\n",
    "#    losses.append(loss)\n",
    "#    print('Fold ' + str(i) + ' model loss: ' + str(loss))\n",
    "#    print('')\n",
    "#    if loss < 0.68:\n",
    "#        models.append(model)\n",
    "#        i += 1\n",
    "#    else:\n",
    "#        print('')\n",
    "#        print('RETRAINING')\n",
    "#        print('############################')\n",
    "#    K.clear_session()\n",
    "#    #del model\n",
    "#    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
